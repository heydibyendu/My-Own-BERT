{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AP3FTX7wzjs"
      },
      "source": [
        "**Training a BERT Model from Scratch**\n",
        "\n",
        "\n",
        "We will train our own BERT model from scratch. The model, myBERT, will be trained as a small model with 6 layers, 12 heads, and about 83 million parameters.\n",
        "\n",
        "myBERT will be a DistillBERT-like model as it has the same architechture of 6 layers and 12 heads.\n",
        "\n",
        "myBERT will implement a byte-level byte-pair encoding tokenizer (used by GPT-2). BERT models mostly use a workpiece tokenizer. There will be no token type IDs as the segment will be separated by the separation token </s>.\n",
        "\n",
        "We'll use use the complete works of William Shakespeare as our dataset, train a tokenizer, train the transformer, save it, and run it with a masked language modeling examples.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdE7IaC9a5GE"
      },
      "source": [
        "**1) Loading the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQzd5FiqWYRh"
      },
      "source": [
        "Let's download 'The Complete Works of William Shakespeare' as a single text file from Project Gutenberg. We can use snippets from this file as the training data for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyDXd7Sxat9Z",
        "outputId": "1371c165-290b-4cf3-d1d4-71362b05c632"
      },
      "source": [
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-08 13:48:57--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/100/100-0.txt [following]\n",
            "--2021-06-08 13:48:58--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5757108 (5.5M) [text/plain]\n",
            "Saving to: â€˜/content/shakespeare.txtâ€™\n",
            "\n",
            "/content/shakespear 100%[===================>]   5.49M  3.59MB/s    in 1.5s    \n",
            "\n",
            "2021-06-08 13:49:00 (3.59 MB/s) - â€˜/content/shakespeare.txtâ€™ saved [5757108/5757108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX0-CRxoWJO8",
        "outputId": "2fb7cd43-69ba-4843-bc3f-460185b946d7"
      },
      "source": [
        "# Get a glimpse of the dataset\n",
        "!head -n5 /content/shakespeare.txt\n",
        "!echo \"...\"\n",
        "!shuf -n5 /content/shakespeare.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ï»¿The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "...\n",
            "    was put down, and the worser allow'd by order of law a furr'd\n",
            "    I'll steal away.\n",
            "MESSENGER.\n",
            "    Grace!\n",
            "What doth our cousin lay to Mowbrayâ€™s charge?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgnM7ylZWw6Q"
      },
      "source": [
        "#SHAKESPEARE_TXT = '/content/shakespeare.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH19JH5NbOLP"
      },
      "source": [
        "**2) Installing Hugging Face Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ywkL7oaMAV"
      },
      "source": [
        "We need HuggingFace transformers and tokenizer; however, we don't need tensorflow for our task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlyOV07Rcn_3",
        "outputId": "88bdf7de-c305-404f-9b22-1f82c6d6550e"
      },
      "source": [
        "!pip uninstall -y tensorflow #won't need tensoflow here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Nqzf1ScoVY",
        "outputId": "7c1b0f0f-15c3-4734-da22-bc5ce0d01439"
      },
      "source": [
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-gal6mr23\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-gal6mr23\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==4.7.0.dev0 from git+https://github.com/huggingface/transformers in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2020.12.5)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.7.0.dev0-cp37-none-any.whl size=2351635 sha256=7aeabe6a2ae494f6a9edcb44a7dc618a5cc2622f79ec5bafb669a109e56b7838\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cm_ea8p7/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEb3cRVRdP2f",
        "outputId": "f2a567c8-e713-4e92-b5c3-6ac59b2e90c1"
      },
      "source": [
        "# Check the versions\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizers                    0.10.3             \n",
            "transformers                  4.7.0.dev0         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYWQvV_ObdPs"
      },
      "source": [
        "**3) Training a Tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbnE5Zj5bOTk"
      },
      "source": [
        "We'll also train a tokenizer from scratch. We'll be using a byte-level tokenizer, which breaks a string/word into a sub-string/sub-word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EularVDVdPHB"
      },
      "source": [
        "# Install BPE Tokenizer\n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2GIMXG4dPKV",
        "outputId": "08457e15-58ad-43b5-f3c9-a63c98cd7ef9"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Select the files\n",
        "paths = [str(x) for x in Path('.').glob('**/*.txt')]\n",
        "\n",
        "# Select vocab size\n",
        "vocab_size = 52_000\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths,\n",
        "                vocab_size=vocab_size, # size of our tokenizer's model length\n",
        "                min_frequency=2, # minimum frequency threshold\n",
        "                special_tokens=[\n",
        "                                '<s>',    # a start token\n",
        "                                '<pad>',  # a padding token\n",
        "                                '</s>',   # an end token\n",
        "                                '<unk>',  # an unknown token\n",
        "                                '<mask>', # the mask token for language modeling\n",
        "                ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.36 s, sys: 337 ms, total: 4.7 s\n",
            "Wall time: 2.53 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlzPlAR2bdSm"
      },
      "source": [
        "**4) Saving the files to disk**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-R-LfSsdUpy"
      },
      "source": [
        "The tokenizer generates two files after training:\n",
        "1) merges.txt: contains merged tokenized sub-strings\n",
        "2) vocab.json: contains the indices of the tokenize sub-strings\n",
        "\n",
        "Let's create 'myBERT' directory and save the 2 files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rACG-lyfMna",
        "outputId": "703a6424-11df-464d-d6b5-3fe76d548062"
      },
      "source": [
        "import os\n",
        "\n",
        "token_dir = '/content/myBERT'\n",
        "\n",
        "if not os.path.exists(token_dir):\n",
        "  os.makedirs(token_dir)\n",
        "\n",
        "tokenizer.save_model('myBERT')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['myBERT/vocab.json', 'myBERT/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CENCgVMwbdVT"
      },
      "source": [
        "**5) Loading the Trained Tokenizer Files**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y4Yr8oPebMH"
      },
      "source": [
        "Since we have trained our own tokenizer, let's load the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HbjxJCyf1Ji"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer=ByteLevelBPETokenizer(\n",
        "    './myBERT/vocab.json',\n",
        "    './myBERT/merges.txt'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIsK1PTTgtnD",
        "outputId": "8dc99d79-0af7-4a72-abd3-d648d1a1c83a"
      },
      "source": [
        "# Let's encode a post-processed sequence\n",
        "tokenizer.encode('nference is inference without bias')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRwy0gemf1Tx",
        "outputId": "1f78d7e7-d851-4466-a62c-72ae05d943c5"
      },
      "source": [
        "# Check how the tokenizer works\n",
        "tokenizer.encode('nference is inference without bias').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['n', 'ference', 'Ä is', 'Ä in', 'ference', 'Ä without', 'Ä bias']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48B-V2Hoh-Mo"
      },
      "source": [
        "Now let the tokenizer process the tokens to fit the BERT model. The post processor will add a start and end token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOyjF2zDg620"
      },
      "source": [
        "tokenizer._tokenizer.post_processor=BertProcessing(\n",
        "    ('</s>', tokenizer.token_to_id('</s>')),\n",
        "    ('<s>', tokenizer.token_to_id('<s>'))\n",
        ")\n",
        "\n",
        "# Select model dimension\n",
        "d_model = 512\n",
        "\n",
        "tokenizer.enable_truncation(max_length=d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df6r1bkuhSSf",
        "outputId": "0065d293-c063-4403-dfae-95689c0caca5"
      },
      "source": [
        "# Let's encode a post-processed sequence\n",
        "tokenizer.encode('nference is inference without bias')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZwZHW-SieO2"
      },
      "source": [
        "After the post-process, we can see that the token size has increased to 9 because the tokenizer added start and end tokens (below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGodPzE5hScp",
        "outputId": "060feb1f-7caa-49f6-dd46-44f0329c2212"
      },
      "source": [
        "# Check how the tokenizer works\n",
        "tokenizer.encode('nference is inference without bias').tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'n', 'ference', 'Ä is', 'Ä in', 'ference', 'Ä without', 'Ä bias', '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoL0-7S6bdX-"
      },
      "source": [
        "**6) Checking GPU and NVIDIA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaDpiuNxsFhk",
        "outputId": "f6fc58d2-a187-4de5-e78c-d0919898e3c6"
      },
      "source": [
        "#Let's see if an NVIDIA GPU card is present\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun  3 16:34:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    34W /  70W |  11344MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6HWumfijvPT"
      },
      "source": [
        "We can see the GPU information as well as the version on the card. We'll now check if PyTorch sees CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qcix4jlsFlv",
        "outputId": "37c7a3be-d9fb-464a-db05-d3bc0a645580"
      },
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3HNoJtebda7"
      },
      "source": [
        "**7) Defining the configuration of the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0L0n8Mwkh0l"
      },
      "source": [
        "We'll pretrain a RoBERTa-type transformer using same number of layers and heads as a DistillBERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0xxZOEVuh9G"
      },
      "source": [
        "# Import Roberta configurations\n",
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=512,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh_7gS8IuiDh",
        "outputId": "905dda2e-0206-41d5-dba7-76f8e1b826e4"
      },
      "source": [
        "config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.7.0.dev0\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 52000\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj4aCud_bddj"
      },
      "source": [
        "**8) Re-creating the Tokenizer in Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS8JzJxVl4i5"
      },
      "source": [
        "Now let's load our trained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emH4eCBwmNz"
      },
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('./myBERT', max_length=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVc7gz1emGdy"
      },
      "source": [
        "Since we have loaded our trained tokenizer, let's initialize a RoBERTa model from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO2I9ukdbdgc"
      },
      "source": [
        "**9) Initializing a Model From Scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mh0aGN0ydv8",
        "outputId": "c45bbea6-d5f6-493b-fac3-98fceb0fc548"
      },
      "source": [
        "# Import RoBERTa masked model\n",
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "# Initiate the model with defined configurations\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "\n",
        "# Print the model and see its building blocks\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke8_HyEjyd4f",
        "outputId": "1bc404b2-8613-4ad7-9298-a8075ef147b4"
      },
      "source": [
        "# Check num of parameters\n",
        "model.num_parameters()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83502880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6cYeWpnowba"
      },
      "source": [
        "It's a small model, only with 83 million parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvLAj2A_bdjb"
      },
      "source": [
        "**10) Building the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVSLcrMwpIXo"
      },
      "source": [
        "Let's load the dataset line by line for batch training with the block size of 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha8mpYZFzTez",
        "outputId": "4f9b022c-f18c-491b-d6f5-eb15b3f73e25"
      },
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='./shakespeare.txt',\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs_aiwQgbdmT"
      },
      "source": [
        "**11) Defining a Data Collator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv6OY0E7qFEr"
      },
      "source": [
        "Before initializing the trainer, we need to run a data collator, which takes samples from the dataset and collate them into batches (dictionary-like objects)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_75xIhb0KLN",
        "outputId": "e401ef70-f6f4-4a36-ed66-b68d9768cb2e"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,             # pretraining for masked language modeling\n",
        "    mlm_probability=0.15  # proportion of masked tokens\n",
        ")\n",
        "\n",
        "print('Done...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl_pd36bbdpR"
      },
      "source": [
        "**12) Initializing the Trainer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLcDSxCr0KQQ",
        "outputId": "51037e72-e2d7-4634-b53f-73f500a77feb"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Set training arguments\n",
        "training_args=TrainingArguments(\n",
        "    output_dir='./myBERT',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10000,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "print('Done...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yoe4trf1cJA",
        "outputId": "ad3d7abe-caa6-4171-d0ef-3256a6c0aac4"
      },
      "source": [
        "# Set the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "print('The model is ready for training...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model is ready for training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqQ3Atasbdsd"
      },
      "source": [
        "**13) Pre-training the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPID8NSCq2C_"
      },
      "source": [
        "All set; now launch the trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "QjdG4sde1bUK",
        "outputId": "7d4887c2-6c1c-4bd3-969f-a29bbb9c2a43"
      },
      "source": [
        "%%time\n",
        "trainer.train()\n",
        "# done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2206' max='2206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2206/2206 09:06, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.020100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>6.142500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.894100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>5.719100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8min 52s, sys: 9.67 s, total: 9min 2s\n",
            "Wall time: 9min 6s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2206, training_loss=6.147931677367832, metrics={'train_runtime': 546.5641, 'train_samples_per_second': 258.283, 'train_steps_per_second': 4.036, 'total_flos': 1518795807006720.0, 'train_loss': 6.147931677367832, 'epoch': 1.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yeh_oiXGrCC5"
      },
      "source": [
        "We can see the training process in real time, inclusing the loss, learning rate, epoch and steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXynjBYRcXrf"
      },
      "source": [
        "**14) Saving the Final Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y98Qts7irYGr"
      },
      "source": [
        "The model is trained. It's time to save the model and configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZim2TXn5jXq"
      },
      "source": [
        "trainer.save_model('./myBERT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICeMg3JXruFi"
      },
      "source": [
        "Inside the 'myBERT' folder, we can now see 'config.json, pytorch_model.bin, and training_args.bin' files, where 'vocab.json and merges.txt' contain the pretrained tokenization of our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW1jQFifsbIO"
      },
      "source": [
        "**We have built our BERT model from scratch!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anh50X2qcXua"
      },
      "source": [
        "**15) Language Modeling with the FillMaskPipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3zHBfehsowH"
      },
      "source": [
        "Let's perform masked language modeling using 'fill-mask' pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56MjguPFaymp"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./myBERT\",\n",
        "    tokenizer=\"./myBERT\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASLdiXUR5-Tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b88614-ad8d-4457-eed2-cf6a5e5fafd1"
      },
      "source": [
        "# Let's ask our model to think like Shakespeare :)\n",
        "fill_mask(\"Everything that glitters is not<mask>.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.017595890909433365,\n",
              "  'sequence': 'Everything that glitters is not it.',\n",
              "  'token': 352,\n",
              "  'token_str': ' it'},\n",
              " {'score': 0.016499634832143784,\n",
              "  'sequence': 'Everything that glitters is not me.',\n",
              "  'token': 330,\n",
              "  'token_str': ' me'},\n",
              " {'score': 0.00985712744295597,\n",
              "  'sequence': 'Everything that glitters is not him.',\n",
              "  'token': 368,\n",
              "  'token_str': ' him'},\n",
              " {'score': 0.008378772996366024,\n",
              "  'sequence': 'Everything that glitters is not you.',\n",
              "  'token': 298,\n",
              "  'token_str': ' you'},\n",
              " {'score': 0.006181748118251562,\n",
              "  'sequence': 'Everything that glitters is not thee.',\n",
              "  'token': 448,\n",
              "  'token_str': ' thee'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpAR1JZt6UBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cef9f3-0575-4717-dbfc-3817aca110d9"
      },
      "source": [
        "# Once more: ask our model to think like Shakespeare :)\n",
        "fill_mask(\"To be, or not to<mask>.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.014091857708990574,\n",
              "  'sequence': 'To be, or not to you.',\n",
              "  'token': 298,\n",
              "  'token_str': ' you'},\n",
              " {'score': 0.01212971843779087,\n",
              "  'sequence': 'To be, or not to me.',\n",
              "  'token': 330,\n",
              "  'token_str': ' me'},\n",
              " {'score': 0.01062318030744791,\n",
              "  'sequence': 'To be, or not to it.',\n",
              "  'token': 352,\n",
              "  'token_str': ' it'},\n",
              " {'score': 0.009581162594258785,\n",
              "  'sequence': 'To be, or not to sir.',\n",
              "  'token': 548,\n",
              "  'token_str': ' sir'},\n",
              " {'score': 0.008413019590079784,\n",
              "  'sequence': 'To be, or not to lord.',\n",
              "  'token': 491,\n",
              "  'token_str': ' lord'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbP0wUkqtj22"
      },
      "source": [
        "The output may vary in each run as we're pretraining our model from scratch with limited data (only 5.5 MB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od54EFBTt5T7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}